---
title: "ADS502 Group Project"
author: "Francisco Hernandez, Jason Morfin, Brendan Robinson, Aaron Gabriel"
date: "2025-04-02"
output: 
  pdf_document: default
---
```{r setup, include=FALSE}
library(tidyverse)
library(psych) 
library(ggplot2)
library(gridExtra)
library(corrplot)
drug_induced_training_data <- read.csv("/Users/jasonavalos/Projects/school/ADS502/project/Applied-Data-Science/ADS502/drug_induced_autoimmunity_prediction/DIA_trainingset_RDKit_descriptors.csv")
```
##### Data Cleaning
```{r}
threshold <- 0.5
drug_induced_training_data <- drug_induced_training_data[, colMeans(is.na(drug_induced_training_data)) <= threshold]

get_mode <- function(x) {
    uniq_vals <- unique(x)
    uniq_vals[which.max(tabulate(match(x, uniq_vals)))]
}
categorical_cols <- sapply(drug_induced_training_data, is.character)
drug_induced_training_data[categorical_cols] <- lapply(
    drug_induced_training_data[categorical_cols], 
    function(x) {
        x[is.na(x)] <- get_mode(x)
        return(x)
    })
write.csv(drug_induced_training_data, "DIA_trainingset_RDKit_descriptors.csv")
```

##### Descriptive Statistics
```{r}
summary(drug_induced_training_data[, 1:6])
drug_data <- select(drug_induced_training_data, where(is.numeric))
drug_stats <- describe(drug_data)
head(drug_stats, 10)
```

##### Data Quality Report
```{r}
data_quality_report <- data.frame(
  Variable = names(drug_induced_training_data),
  Type = sapply(drug_induced_training_data, class),
  Missing = sapply(drug_induced_training_data, function(x) sum(is.na(x))),
  Complete = sapply(drug_induced_training_data, function(x) sum(!is.na(x))),
  Unique = sapply(drug_induced_training_data, function(x) length(unique(x)))
)
knitr::kable(head(data_quality_report, 40), caption = "Drug Induced Data Quality Report (Preview)")
```


#### Univariate Analysis

```{r}
hist <- ggplot(drug_induced_training_data, aes(x=BalabanJ)) + geom_histogram(bins = 30)
boxplot <- ggplot(drug_induced_training_data, aes(x=BalabanJ)) + geom_boxplot()

grid.arrange(hist, boxplot, ncol = 2)

hist_2 <- ggplot(drug_induced_training_data, aes(x=Chi0n)) + geom_histogram(bins = 30)
boxplot_2 <- ggplot(drug_induced_training_data, aes(x=Chi0n)) + geom_boxplot()

grid.arrange(hist_2, boxplot_2, ncol = 2)

hist_3 <- ggplot(drug_induced_training_data, aes(x=EState_VSA10)) + geom_histogram(bins = 30)
boxplot_3 <- ggplot(drug_induced_training_data, aes(x=EState_VSA10)) + geom_boxplot()

grid.arrange(hist_3, boxplot_3, ncol = 2)

hist_4 <- ggplot(drug_induced_training_data, aes(x=Kappa3)) + geom_histogram(bins = 30)
boxplot_4 <- ggplot(drug_induced_training_data, aes(x=Kappa3)) + geom_boxplot()

grid.arrange(hist_4, boxplot_4, ncol = 2)

hist_5 <- ggplot(drug_induced_training_data, aes(x=PEOE_VSA14)) + geom_histogram(bins = 30)
boxplot_5 <- ggplot(drug_induced_training_data, aes(x=PEOE_VSA14)) + geom_boxplot()

grid.arrange(hist_5, boxplot_5, ncol = 2)
```
Features are right sweked. Few outliers that do not need to be removed.

#### Multivariate Analysis
```{r}
numeric_cols_indexes <- sapply(drug_induced_training_data, is.numeric)
numeric_features <- drug_induced_training_data[numeric_cols_indexes]

data <- as.matrix(numeric_features)

heatmap(data,Colv = NA, Rowv = NA, scale = "row")
```

#### Class Imbalance

```{r}
print("Pre-balance class cardinality")
table(drug_induced_training_data$Label)

# Random Undersampling
balanced_0 <- sample_n(filter(drug_induced_training_data, Label == 0), 118)
filtered_1 <- filter(drug_induced_training_data, Label == 1)
bal_drug_induced_training_data <- rbind(balanced_0, filtered_1)

print("Post-balance class cardinality")
table(bal_drug_induced_training_data$Label)
```


#### Pre-Model Data Preparation
```{r}
# Drop SMILES feature
bal_drug_induced_training_data <- bal_drug_induced_training_data[, colnames(bal_drug_induced_training_data) != "SMILES"]

for (col_name in colnames(bal_drug_induced_training_data)){
# For binary responses
  if (length(unique(bal_drug_induced_training_data[[col_name]])) == 2){
    bal_drug_induced_training_data[[col_name]] <- as.factor(bal_drug_induced_training_data[[col_name]])
  }
  
  # For standarization of numeric responses
  if (is.numeric(bal_drug_induced_training_data[[col_name]])){
    if( max(bal_drug_induced_training_data[[col_name]]) != min(bal_drug_induced_training_data[[col_name]])){
    bal_drug_induced_training_data[[col_name]] <- (bal_drug_induced_training_data[[col_name]] - min(bal_drug_induced_training_data[[col_name]])) /
    (max(bal_drug_induced_training_data[[col_name]]) - min(bal_drug_induced_training_data[[col_name]]))
    }
    else{
          bal_drug_induced_training_data[[col_name]] <- 0
    }
  }
}
# Final Dataset
# bal_drug_induced_training_data
```


#### Feature Selection
```{r}
# Missing step --> numeric_features need to be obtained from bal_drug_induced_training_data

model_all <- lm(Label ~ ., data=numeric_features)  # with all the independent variables in the dataframe to find correlated features
# summary(model_all)
coeff <- coefficients(model_all)

# To remove correlated where coefficient = NaN -> Correlation
na_coeff_names <- names(coeff)[is.na(coeff)]
na_coeff_names

```


# Train and Test

# Load library for modeling and evaluation
library(caret)

# Drop features with NA coefficients
cleaned_data <- bal_drug_induced_training_data[, !(names(bal_drug_induced_training_data) %in% na_coeff_names)]

# Ensure Label is a factor
cleaned_data$Label <- as.factor(cleaned_data$Label)

# Split data into train (80%) and test (20%)
set.seed(123)
train_index <- createDataPartition(cleaned_data$Label, p = 0.8, list = FALSE)
train_data <- cleaned_data[train_index, ]
test_data <- cleaned_data[-train_index, ]


# Baseline Model - Logistic Regression

# Train logistic regression model
log_model <- glm(Label ~ ., data = train_data, family = "binomial")

# Predict probabilities on test data
log_probs <- predict(log_model, newdata = test_data, type = "response")

# Convert probabilities to class labels
log_preds <- ifelse(log_probs > 0.5, 1, 0)

# Evaluate model
confusionMatrix(as.factor(log_preds), test_data$Label, positive = "1")


# Train Other Models (Naive Bayes, Random Forest, C5.0)

# Load libraries
library(e1071)
library(randomForest)
library(C50)

# Naive Bayes
nb_model <- naiveBayes(Label ~ ., data = train_data)
nb_preds <- predict(nb_model, newdata = test_data)
confusionMatrix(nb_preds, test_data$Label)

# Random Forest
rf_model <- randomForest(Label ~ ., data = train_data, ntree = 100)
rf_preds <- predict(rf_model, newdata = test_data)
confusionMatrix(rf_preds, test_data$Label)

# C5.0
c50_model <- C5.0(Label ~ ., data = train_data)
c50_preds <- predict(c50_model, newdata = test_data)
confusionMatrix(c50_preds, test_data$Label)


# Compare Accuracy of All Models

accuracy_summary <- data.frame(
  Model = c("Logistic Regression", "Naive Bayes", "Random Forest", "C5.0"),
  Accuracy = c(
    confusionMatrix(as.factor(log_preds), test_data$Label)$overall["Accuracy"],
    confusionMatrix(nb_preds, test_data$Label)$overall["Accuracy"],
    confusionMatrix(rf_preds, test_data$Label)$overall["Accuracy"],
    confusionMatrix(c50_preds, test_data$Label)$overall["Accuracy"]
  )
)

print(accuracy_summary)








