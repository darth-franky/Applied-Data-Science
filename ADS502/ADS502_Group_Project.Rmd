---
title: "ADS502 Group Project"
author: "Francisco Hernandez, Jason Morfin, Brendan Robinson, Aaron Gabriel"
date: "2025-04-02"
output: 
  pdf_document: default
---
```{r setup, include=FALSE}
library(tidyverse)
library(psych) 
library(ggplot2)
library(gridExtra)
library(corrplot)
library(reshape2)
drug_induced_training_data <- read.csv("/Users/jasonavalos/Projects/school/ADS502/project/Applied-Data-Science/ADS502/drug_induced_autoimmunity_prediction/DIA_trainingset_RDKit_descriptors.csv")
```
##### Data Cleaning
```{r}
threshold <- 0.5
drug_induced_training_data <- drug_induced_training_data[, colMeans(is.na(drug_induced_training_data)) <= threshold]

get_mode <- function(x) {
    uniq_vals <- unique(x)
    uniq_vals[which.max(tabulate(match(x, uniq_vals)))]
}
categorical_cols <- sapply(drug_induced_training_data, is.character)
drug_induced_training_data[categorical_cols] <- lapply(
    drug_induced_training_data[categorical_cols], 
    function(x) {
        x[is.na(x)] <- get_mode(x)
        return(x)
    })
write.csv(drug_induced_training_data, "DIA_trainingset_RDKit_descriptors.csv")
```

##### Descriptive Statistics
```{r}
summary(drug_induced_training_data[, 1:6])
drug_data <- select(drug_induced_training_data, where(is.numeric))
drug_stats <- describe(drug_data)
head(drug_stats, 10)
```

##### Data Quality Report
```{r}
data_quality_report <- data.frame(
  Variable = names(drug_induced_training_data),
  Type = sapply(drug_induced_training_data, class),
  Missing = sapply(drug_induced_training_data, function(x) sum(is.na(x))),
  Complete = sapply(drug_induced_training_data, function(x) sum(!is.na(x))),
  Unique = sapply(drug_induced_training_data, function(x) length(unique(x)))
)
knitr::kable(head(data_quality_report, 40), caption = "Drug Induced Data Quality Report (Preview)")
```


#### Univariate Analysis

```{r}
hist <- ggplot(drug_induced_training_data, aes(x=BalabanJ)) + geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "BalabanJ Histogram", x = "BalabanJ Index", y = "Frecuency")
boxplot <- ggplot(drug_induced_training_data, aes(x=BalabanJ)) + geom_boxplot() +
labs(title = "BalabanJ Boxplot", x = "", y = "BalabanJ Index")

grid.arrange(hist, boxplot, ncol = 2, top="Balabanj Index Distribution")

hist_2 <- ggplot(drug_induced_training_data, aes(x=Chi0n)) + geom_histogram(bins = 30, fill = "steelblue", color = "black")
boxplot_2 <- ggplot(drug_induced_training_data, aes(x=Chi0n)) + geom_boxplot() +
  labs(title = "Chi0n Boxplot", x = "", y = "Chi0n Index")

grid.arrange(hist_2, boxplot_2, ncol = 2)

hist_3 <- ggplot(drug_induced_training_data, aes(x=EState_VSA10)) + geom_histogram(bins = 30, fill = "steelblue", color = "black")
boxplot_3 <- ggplot(drug_induced_training_data, aes(x=EState_VSA10)) + geom_boxplot() +
  labs(title = "EState_VSA10 Boxplot", x = "", y = "EState_VSA10 Index")

grid.arrange(hist_3, boxplot_3, ncol = 2)

hist_4 <- ggplot(drug_induced_training_data, aes(x=Kappa3)) + geom_histogram(bins = 30, fill = "steelblue", color = "black")
boxplot_4 <- ggplot(drug_induced_training_data, aes(x=Kappa3)) + geom_boxplot() +
  labs(title = "Kappa3 Boxplot", x = "", y = "Kappa3 Index")

grid.arrange(hist_4, boxplot_4, ncol = 2)
```
Features are right sweked. Few outliers that do not need to be removed.

```{r}
hist_6 <- ggplot(drug_induced_training_data, aes(x =Label)) +
  geom_bar(fill = "steelblue", color = "black") +
  labs(title = "Target Classes Counts")
hist_6
```

#### Multivariate Analysis
##### Top 10 correlated features
```{r}
numeric_data <- drug_induced_training_data[, sapply(drug_induced_training_data, is.numeric)]
numeric_data <- numeric_data[, apply(numeric_data, 2, sd, na.rm = TRUE) != 0]
numeric_data <- na.omit(numeric_data)

cor_matrix <- cor(numeric_data)
cor_matrix[lower.tri(cor_matrix, diag = TRUE)] <- NA  
cor_long <- melt(cor_matrix, na.rm = TRUE)
top_corr <- cor_long[order(-abs(cor_long$value)), ]

head(top_corr, 10)
```

#### Class Imbalance

```{r}
print("Pre-balance class cardinality")
table(drug_induced_training_data$Label)

# Random Undersampling
balanced_0 <- sample_n(filter(drug_induced_training_data, Label == 0), 118)
filtered_1 <- filter(drug_induced_training_data, Label == 1)
bal_drug_induced_training_data <- rbind(balanced_0, filtered_1)

print("Post-balance class cardinality")
table(bal_drug_induced_training_data$Label)
```


#### Pre-Model Data Preparation
```{r}
# Drop SMILES feature
bal_drug_induced_training_data <- bal_drug_induced_training_data[, colnames(bal_drug_induced_training_data) != "SMILES"]

for (col_name in colnames(bal_drug_induced_training_data)){
# For binary responses
  if (length(unique(bal_drug_induced_training_data[[col_name]])) == 2){
    bal_drug_induced_training_data[[col_name]] <- as.factor(bal_drug_induced_training_data[[col_name]])
  }
  
  # For standarization of numeric responses
  if (is.numeric(bal_drug_induced_training_data[[col_name]])){
    if( max(bal_drug_induced_training_data[[col_name]]) != min(bal_drug_induced_training_data[[col_name]])){
    bal_drug_induced_training_data[[col_name]] <- (bal_drug_induced_training_data[[col_name]] - min(bal_drug_induced_training_data[[col_name]])) /
    (max(bal_drug_induced_training_data[[col_name]]) - min(bal_drug_induced_training_data[[col_name]]))
    }
    else{
          bal_drug_induced_training_data[[col_name]] <- 0
    }
  }
}
# Final Dataset
# bal_drug_induced_training_data
```


#### Feature Selection
```{r}
# Missing step --> numeric_features need to be obtained from bal_drug_induced_training_data
numeric_cols_indexes <- sapply(drug_induced_training_data, is.numeric)
numeric_features <- drug_induced_training_data[numeric_cols_indexes]

model_all <- lm(Label ~ ., data=numeric_features)  # with all the independent variables in the dataframe to find correlated features
# summary(model_all)
coeff <- coefficients(model_all)

# To remove correlated where coefficient = NaN -> Correlation
na_coeff_names <- names(coeff)[is.na(coeff)]
na_coeff_names

```
# Train and Test
```{r}
# Remove pound if you haven't installed packages below
#loading libraries for modeling and evaluation
# install.packages("hardhat")
# install.packages("parallelly")
# install.packages("caret")

library(caret)

# Drop features with NA coefficients
cleaned_data <- bal_drug_induced_training_data[, !(names(bal_drug_induced_training_data) %in% na_coeff_names)]

# Ensure Label is a factor
cleaned_data$Label <- as.factor(cleaned_data$Label)

# Split data into train (80%) and test (20%)
set.seed(123)
train_index <- createDataPartition(cleaned_data$Label, p = 0.8, list = FALSE)
train_data <- cleaned_data[train_index, ]
test_data <- cleaned_data[-train_index, ]
```

```{r}
# determining constant variables for removal
constant_vars <- c("fr_epoxide", "fr_morpholine", "fr_oxime", "fr_tetrazole")

# removing from the training and test datasets:
train_data_fixed <- train_data[, !(names(train_data) %in% constant_vars)]
test_data_fixed  <- test_data[, !(names(test_data) %in% constant_vars)]

# Remove the constant variable fr_oxazole from both training and test data
train_data_fixed <- train_data_fixed[, !(names(train_data_fixed) %in% c("fr_oxazole"))]
test_data_fixed  <- test_data_fixed[, !(names(test_data_fixed) %in% c("fr_oxazole"))]

# removes factor columns with only one level
train_data_fixed <- train_data_fixed[, sapply(train_data_fixed, function(col) {
  !(is.factor(col) || is.character(col)) || length(unique(col)) > 1
})]

```

```{r}

# Baseline Model - Logistic Regression

# Train logistic regression model
log_model <- glm(Label ~ ., data = train_data_fixed, family = "binomial")

# Predict probabilities on test data
log_probs <- predict(log_model, newdata = test_data_fixed, type = "response")

# Convert probabilities to class labels
log_preds <- ifelse(log_probs > 0.5, 1, 0)

# Evaluate model
confusionMatrix(as.factor(log_preds), test_data_fixed$Label, positive = "1")
```

```{r}
# Remove pound if you haven't installed packages below
# install.packages("e1071")
# install.packages("randomForest")
# install.packages("C50")
```


```{r}
# Train Other Models (Naive Bayes, Random Forest, C5.0)

# Load libraries
library(e1071)
library(randomForest)
library(C50)

# Naive Bayes
nb_model <- naiveBayes(Label ~ ., data = train_data)
nb_preds <- predict(nb_model, newdata = test_data)
confusionMatrix(nb_preds, test_data$Label)

# Random Forest
rf_model <- randomForest(Label ~ ., data = train_data, ntree = 100)
rf_preds <- predict(rf_model, newdata = test_data)
confusionMatrix(rf_preds, test_data$Label)

# C5.0
c50_model <- C5.0(Label ~ ., data = train_data)
c50_preds <- predict(c50_model, newdata = test_data)
confusionMatrix(c50_preds, test_data$Label)
```

```{r}
# creating a function to extract evaluation metrics from confusion matrix
get_metrics <- function(preds, actual) {
  cm <- confusionMatrix(as.factor(preds), as.factor(actual), positive = "1")
  c(
    Accuracy = cm$overall["Accuracy"],
    Sensitivity = cm$byClass["Sensitivity"],
    Specificity = cm$byClass["Specificity"],
    Precision = cm$byClass["Pos Pred Value"]
  )
}

# building model evaluation table for model comparison
accuracy_summary <- data.frame(
  Model = c("Logistic Regression", "Naive Bayes", "Random Forest", "C5.0"),
  rbind(
    get_metrics(log_preds, test_data$Label),
    get_metrics(nb_preds, test_data$Label),
    get_metrics(rf_preds, test_data$Label),
    get_metrics(c50_preds, test_data$Label)
  )
)

# View result
print(accuracy_summary)

```